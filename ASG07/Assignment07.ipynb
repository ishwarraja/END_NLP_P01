{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIVr1LnIQrvF"
      },
      "source": [
        "# Session7-Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPJH-p5vcegS"
      },
      "source": [
        "import torch\n",
        "import torchtext \n",
        "from torchtext import data, datasets\n",
        "import pandas as pd\n",
        "import nltk\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-pvKCM8KGOP",
        "outputId": "d42261c2-6752-4fdc-fc9f-03daaa48f4fb"
      },
      "source": [
        "df_dict = pd.read_csv(\"dictionary.txt\", sep='\\|', header=None)\n",
        "df_label = pd.read_csv(\"sentiment_labels.txt\", sep='|')\n",
        "df_sentence = pd.read_csv(\"datasetSentences.txt\", sep='\\t')\n",
        "df_split = pd.read_csv(\"datasetSplit.txt\",sep=',')\n",
        "df_org =pd.read_csv(\"original_rt_snippets.txt\", sep='\\t')\n",
        "\n",
        "#df_original = pd.read_csv(\"original_rt_snippets.txt\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siqyPp1DKmXR",
        "outputId": "c4cbee8b-cccd-4369-aacb-608f843a6663"
      },
      "source": [
        "print(f\"Dictionary: {df_dict.shape}\")\n",
        "print(f\"Sentimenet: {df_label.shape}\")\n",
        "print(f\"Sentence: {df_sentence.shape}\")\n",
        "print(f\"DataSplit: {df_split.shape}\")\n",
        "print(f\"Org_Snippet: {df_org.shape}\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary: (239232, 2)\n",
            "Sentimenet: (239232, 2)\n",
            "Sentence: (11855, 2)\n",
            "DataSplit: (11855, 2)\n",
            "Org_Snippet: (10604, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeWB709pLaGg",
        "outputId": "a0a4e0c4-e77e-4d53-f3d5-b1e26532b7c5"
      },
      "source": [
        "len(df_dict), len(df_label),len(df_sentence), len(df_split)\n",
        "\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(239232, 239232, 11855, 11855)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIx9W65zMFp6"
      },
      "source": [
        "df_dict.columns = ['phrases', 'phrase_id']"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "M6P7GKhzI_le",
        "outputId": "55332dc0-ef6f-4172-ddf1-ccc96568f05c"
      },
      "source": [
        "# Merging DataSentence & DataSplit with common column sentence_index\n",
        "# df_sentence.columns, df_split.columns\n",
        "DS01_merge = df_sentence.merge(df_split, on='sentence_index')\n",
        "\n",
        "# Merging Dictionary with DataSentence & DataSplit \n",
        "DS02_merge = DS01_merge.merge(df_dict, left_on='sentence', right_on ='phrases', how='left')\n",
        "\n",
        "# Merging label with DS_02_merge having dictionary, datasentence, datasplit\n",
        "#df = pd.merge(df_phrase , df_label, how='inner', left_on=1, right_on='phrase ids' )\n",
        "DS03_merge = DS02_merge.merge(df_label, left_on='phrase_id', right_on='phrase ids', how='left')\n",
        "DS03_merge.head(5)\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>phrases</th>\n",
              "      <th>phrase_id</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>0.69444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>1</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>0.83333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>2</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>0.51389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>2</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>0.73611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>2</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>0.86111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ... sentiment values\n",
              "0               1  ...          0.69444\n",
              "1               2  ...          0.83333\n",
              "2               3  ...          0.51389\n",
              "3               4  ...          0.73611\n",
              "4               5  ...          0.86111\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YClj3p-mNoQE",
        "outputId": "433a1e6f-9145-494a-f01b-7c0dbf298ad2"
      },
      "source": [
        "print(f\"DS03_merge: {DS03_merge.shape}\")\n",
        "print(f\"DS03_merge: {DS03_merge.columns}\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DS03_merge: (11855, 7)\n",
            "DS03_merge: Index(['sentence_index', 'sentence', 'splitset_label', 'phrases', 'phrase_id',\n",
            "       'phrase ids', 'sentiment values'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV94HKOuXoU8"
      },
      "source": [
        "DS03_merge['labels'] = [(1 if 0 <=i <=0.2 else (2 if 0.2<i<=0.4 else (3 if 0.4 <i<=0.6 else (4 if 0.6 <i<=0.8 else 5)))) for i in DS03_merge['sentiment values'] ]\n",
        "DS03_merge.rename(columns={'sentence':'tweets'}, inplace=True)\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk5HVUOec3fB",
        "outputId": "24b85ad9-4489-429f-8bf5-455951b12fd1"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpOy01TkL-Fp",
        "outputId": "506f4d50-8ce8-4111-b0e3-6a9c0894b534"
      },
      "source": [
        "# DATA Augmentation approaches\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stoplist  = set(stopwords.words('english'))\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "punct = list(string.punctuation)\n",
        "punct.remove(\"-\")\n",
        "punct.append(\" \")\n",
        "\n",
        "\n",
        "def get_synonyms(word):\n",
        "  if word.lower() in stop:\n",
        "      return [word], [1]\n",
        "\n",
        "  synonyms = set()\n",
        "  for syn in wordnet.synsets(word): \n",
        "      for l in syn.lemmas(): \n",
        "          synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "          synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "          synonyms.add(synonym) \n",
        "\n",
        "  if word not in synonyms:\n",
        "      synonyms.add(word)\n",
        "      \n",
        "  n = len(synonyms)\n",
        "\n",
        "  if n == 1:\n",
        "      word_ = \"\".join(list(filter(lambda x: x not in punct, word)))\n",
        "      if word_.lower() in stop:\n",
        "          return [word, word_], [0.5, 0.5]\n",
        "      for syn in wordnet.synsets(word_): \n",
        "          for l in syn.lemmas(): \n",
        "              synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "              synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "              synonyms.add(synonym) \n",
        "      if word_ not in synonyms:\n",
        "          synonyms.add(word_)\n",
        "          \n",
        "  n = len(synonyms)\n",
        "  if n == 1:\n",
        "      probabilities = [1]\n",
        "  else:\n",
        "      probabilities = [0.5 if w==word else 0.5/(n-1) for w in synonyms]\n",
        "\n",
        "  return list(synonyms), probabilities\n",
        "\n",
        "def new_row(row, n_samples=1): \n",
        "  text = row['text']\n",
        "  new_rows = [row]\n",
        "  for i in range(n_samples):\n",
        "      row2 = row\n",
        "      new_words = []\n",
        "      for word in text.split():\n",
        "          syns, prob = get_synonyms(word)\n",
        "          selected_word = np.random.choice(syns, p=prob, replace=True)\n",
        "          if type(row['keyword']) == str:\n",
        "              if word != selected_word and word in row['keyword']:\n",
        "                  row2['keyword'] = row2['keyword'].replace(word, selected_word)\n",
        "          new_words.append(selected_word)\n",
        "          \n",
        "      \n",
        "      new_text = \" \".join(new_words)\n",
        "      \n",
        "      row2['text'] = new_text\n",
        "      new_rows.append(row2)\n",
        "\n",
        "  new_rows = pd.concat(new_rows, axis=1).transpose().drop_duplicates(subset=['text'], inplace=False, ignore_index=True)\n",
        "  new_rows = new_rows.loc[new_rows['text'].apply(len)<150]\n",
        "\n",
        "\n",
        "  return new_rows\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  return [word for word in word_tokenize(text) if not word in stoplist]\n",
        "\n",
        "def random_swap(sentence, n=5): \n",
        "  length = range(len(sentence)) \n",
        "  for _ in range(n):\n",
        "      idx1, idx2 = random.sample(length, 2)\n",
        "      sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "  return sentence\n",
        "\n",
        "def random_insertion(sentence, n): \n",
        "  words = remove_stopwords(sentence) \n",
        "  for _ in range(n):\n",
        "      new_synonym = get_synonyms(random.choice(words))\n",
        "      sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "  return sentence"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqO9z7OyTiqJ",
        "outputId": "1f485c07-b82c-490a-c289-ac22ce284862"
      },
      "source": [
        "x, y = get_synonyms('happy')\n",
        "x, y"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['well chosen', 'felicitous', 'happy', 'glad'],\n",
              " [0.16666666666666666, 0.16666666666666666, 0.5, 0.16666666666666666])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE86hSmZTvX7"
      },
      "source": [
        "final_df = DS03_merge[['tweets','labels', 'splitset_label']].copy()\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3ZK9RsBTzre",
        "outputId": "eb2bb980-47e3-4f16-bba5-67d174aaa53d"
      },
      "source": [
        "final_df.shape"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8bCRdLST7E2"
      },
      "source": [
        "train_df, test_df, dev_df = [groups for name, groups in final_df.groupby('splitset_label')]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ15LvW1Wino"
      },
      "source": [
        "def clean_text(x):\n",
        "\n",
        "    x = str(x)\n",
        "    for punct in \"/-'\":\n",
        "        x = x.replace(punct, ' ')\n",
        "    for punct in '&':\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
        "        x = x.replace(punct, '')\n",
        "    return x"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec2YxKQnT-ED"
      },
      "source": [
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "dev_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "train_df['tweets'] = train_df['tweets'].apply(lambda x:clean_text(x))"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bwV7yi5gUCpj",
        "outputId": "b81fbff7-55db-4fb7-fddf-a21faa8cbec6"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century  s...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of  The ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singer composer Bryan Adams contributes a slew...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You  d think by now America would have had eno...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet the act is still charming here</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tweets  labels  splitset_label\n",
              "0  The Rock is destined to be the 21st Century  s...       4               1\n",
              "1  The gorgeously elaborate continuation of  The ...       5               1\n",
              "2  Singer composer Bryan Adams contributes a slew...       4               1\n",
              "3  You  d think by now America would have had eno...       3               1\n",
              "4                Yet the act is still charming here        4               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXjP9QbbVs5n",
        "outputId": "41bc554d-aa29-401d-9143-b1c0851d3a93"
      },
      "source": [
        "# experimenting with one, random_insertion\n",
        "ran_swapped_tweets = random_swap(train_df['tweets'], 5)\n",
        "\n",
        "new_df = pd.DataFrame()\n",
        "new_df['tweets'] = ran_swapped_tweets\n",
        "new_df['labels'] = train_df['labels']\n",
        "#new_df taken from below\n",
        "new_df['splitset_label'] = train_df['splitset_label']"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SYRBoUwWo5v",
        "outputId": "2bf4b5e5-faa8-490c-e86c-3b66214c35c4"
      },
      "source": [
        "train_df.shape, new_df.shape"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8544, 3), (8544, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPzaV7StWtlB",
        "outputId": "493b05ed-55c4-48d5-9801-d5f18e5a4c33"
      },
      "source": [
        "augmented_train_df = pd.concat([train_df[['tweets','labels','splitset_label']], new_df], ignore_index=True)\n",
        "print(\"train_data count\", augmented_train_df.labels.value_counts(), augmented_train_df.shape)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data count 4    4432\n",
            "2    4184\n",
            "5    3314\n",
            "3    3098\n",
            "1    2060\n",
            "Name: labels, dtype: int64 (17088, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q34l9_77aeUb",
        "outputId": "4c94b437-a6e3-4b69-bb6b-de4b4eed8fd6"
      },
      "source": [
        "!pip install nlpaug\n",
        "!pip install fairseq\n",
        "!pip install sacremoses"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.0.4)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.4.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (3.3.0)\n",
            "Requirement already satisfied: omegaconf>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (2.0.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (4.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf>=2.0.5->hydra-core->fairseq) (5.3.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vug4KCmtWzWz"
      },
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "#import fairseq\n",
        "#from fairseq.models.transformer import TransformerModel\n",
        "\n",
        "#aug = naw.BackTranslationAug()\n",
        "#aug = naw.SynonymAug()"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5MtbVAeXE7C"
      },
      "source": [
        "\n",
        "\n",
        "augmented_synonyms = [aug.augment(i,num_thread=32) for i in train_df['tweets']]\n",
        "augment_synonyms_df = pd.DataFrame()\n",
        "augment_synonyms_df['tweets'] = augmented_synonyms\n",
        "augment_synonyms_df['labels'] = train_df['labels']\n",
        "#new_df taken from below\n",
        "augment_synonyms_df['splitset_label'] = train_df['splitset_label']"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msi00uZ7XZjB"
      },
      "source": [
        "augmented_train_df = pd.concat([augmented_train_df, augment_synonyms_df], ignore_index=True)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PStD2-M1a4Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c18f76a-fc64-4cbe-8d95-59f3cce30bb4"
      },
      "source": [
        "augmented_train_df.to_csv(\"augmented_data.csv\")\n",
        "print(\"Completed\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQXER5_pC6Kb",
        "outputId": "f937bc0f-246f-4e21-f115-246ad721cf36"
      },
      "source": [
        "!pip install fastBPE"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.6/dist-packages (0.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKirHA5YbKZu"
      },
      "source": [
        "#aug = naw.BackTranslationAug()"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULXtNF7BbOTc"
      },
      "source": [
        "augment_bt_df = pd.DataFrame()\n",
        "augment_bt_df['tweets'] = train_df['tweets'].apply(lambda x:aug.augment(x))\n",
        "augment_bt_df['labels'] = train_df['labels']\n",
        "#new_df taken from below\n",
        "augment_bt_df['splitset_label'] = train_df['splitset_label']"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96MM2EuybQkZ"
      },
      "source": [
        "augmented_train_df = pd.concat([augmented_train_df, augment_bt_df], ignore_index=True)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGPjeR99c8Vb"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "Tweet = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-cqJXLfFJS"
      },
      "source": [
        "fields = [('tweets', Tweet),('labels',Label)]\n",
        "final_df.head()\n",
        "df = final_df[['tweets','labels']].copy()\n",
        "train_df = augmented_train_df.copy()"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwqKd14bfOKx"
      },
      "source": [
        "train_example = [data.Example.fromlist([train_df.tweets[i],train_df.labels[i]], fields) for i in range(train_df.shape[0])] \n",
        "test_example = [data.Example.fromlist([test_df.tweets[i],test_df.labels[i]], fields) for i in range(test_df.shape[0])] \n",
        "valid_example = [data.Example.fromlist([dev_df.tweets[i],dev_df.labels[i]], fields) for i in range(dev_df.shape[0])] "
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqi-FY0MfQ2O"
      },
      "source": [
        "train = data.Dataset(train_example, fields)\n",
        "valid = data.Dataset(valid_example, fields)\n",
        "test = data.Dataset(test_example, fields)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YFvK78GfTzr",
        "outputId": "48d26bb3-b41f-45f5-b492-abeb5ee6ae12"
      },
      "source": [
        "(len(train), len(valid), len(test))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34176, 1101, 2210)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WpH0SXGfWcu",
        "outputId": "8895a651-d137-4a80-cb4b-335279e2d655"
      },
      "source": [
        "vars(train.examples[10])"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 5,\n",
              " 'tweets': ['Good',\n",
              "  'fun',\n",
              "  ' ',\n",
              "  'good',\n",
              "  'action',\n",
              "  ' ',\n",
              "  'good',\n",
              "  'acting',\n",
              "  ' ',\n",
              "  'good',\n",
              "  'dialogue',\n",
              "  ' ',\n",
              "  'good',\n",
              "  'pace',\n",
              "  ' ',\n",
              "  'good',\n",
              "  'cinematography']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjgCCT6DN5cr",
        "outputId": "c983369c-d78d-42d1-ccd7-f0a22c8059af"
      },
      "source": [
        "#!ls drive/MyDrive/data/.vector_cache\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt     glove.6B.200d.txt  glove.6B.50d.txt\n",
            "glove.6B.100d.txt.pt  glove.6B.300d.txt  glove.6B.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL58WQ2lN_Ca"
      },
      "source": [
        "#glove_path = \"drive/MyDrive/data/.vector_cache\""
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wawAhzc0f1Ky",
        "outputId": "9bd2cdcc-0378-45d1-af5f-270050087dd2"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "Tweet.build_vocab(train, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-16 21:04:43 | INFO | torchtext.vocab | Loading vectors from .vector_cache/glove.6B.100d.txt.pt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAkJ2NUwf6R-",
        "outputId": "c3ee131f-6c80-41c9-f5a8-2f5e051bcd7f"
      },
      "source": [
        "print('Size of input vocab : ', len(Tweet.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  24645\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [(' ', 25142), ('the', 24557), ('of', 18237), ('and', 18067), ('a', 18055), ('to', 12405), ('s', 9853), ('is', 8642), ('that', 7830), ('in', 7552)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7fcae902b400>, {4: 0, 2: 1, 5: 2, 3: 3, 1: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqAwV0rULXyv"
      },
      "source": [
        "Lots Of Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLtYxci8f7zc"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AQ_McJ6LwKp",
        "outputId": "663656a4-7074-49dd-a030-e48540c1f64e"
      },
      "source": [
        "ls -a"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/                    datasetSplit.txt          sentiment_labels.txt\n",
            "\u001b[01;34m..\u001b[0m/                   dictionary.txt            SOStr.txt\n",
            "augmented_data.csv    \u001b[01;34mdrive\u001b[0m/                    STree.txt\n",
            "\u001b[01;34m.config\u001b[0m/              original_rt_snippets.txt  tokenizer.pkl\n",
            "datasetSentences.txt  \u001b[01;34msample_data\u001b[0m/              \u001b[01;34m.vector_cache\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tvl03r4NPrY"
      },
      "source": [
        "cp -r .vector_cache drive/MyDrive/data/.vector_cache\n"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb3AeYBMgCbs"
      },
      "source": [
        "batch_size=64\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = batch_size, \n",
        "                                                            sort_key = lambda x: len(x.tweets),\n",
        "                                                            sort_within_batch=True, device = device)\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits((train, test), batch_size = batch_size, \n",
        "                                                            sort_key = lambda x: len(x.tweets),\n",
        "                                                            sort_within_batch=True, device = device)\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEltufRCgSiG"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Tweet.vocab.stoi, tokens)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO2mdc__Lsgq"
      },
      "source": [
        "Defining Our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMmo5-PQgaIO"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx, bidirectional=False):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        self.bidirectional = bidirectional\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        scaling_factor = 2 if bidirectional else 1\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*scaling_factor, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        if self.bidirectional:\n",
        "          hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "          hidden = self.dropout(hidden)\n",
        "          hidden = hidden[0]\n",
        "          \n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs, dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvmOw0omgfkT"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Tweet.vocab)\n",
        "embedding_dim = 100\n",
        "num_hidden_nodes = 256\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "bidirectional=True\n",
        "PAD_IDX = Tweet.vocab.stoi[Tweet.pad_token]\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout, pad_idx=PAD_IDX, bidirectional=bidirectional)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pzNvJyDgjGz"
      },
      "source": [
        "PAD_IDX = Tweet.vocab.stoi[Tweet.pad_token]\n",
        "\n",
        "UNK_IDX = Tweet.vocab.stoi[Tweet.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK7_H5BEgmhU",
        "outputId": "3bbc3d9e-0d9f-4009-99e4-f0959fb62860"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(24645, 100, padding_idx=1)\n",
            "  (encoder): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n",
            "The model has 4,777,209 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6SbLIa5L5hW"
      },
      "source": [
        "Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZvX15m-gpri"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "# lr=0.005\n",
        "# lr = 2e-4\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_MVu6pIL_43"
      },
      "source": [
        "Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xom9ZHpEg5Gf"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.tweets   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        #acc = binary_accuracy(predictions, batch.labels)   \n",
        "        acc = categorical_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3W5VvCPMGBH"
      },
      "source": [
        "Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxchQFkHg8Q3"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.tweets\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            # acc = binary_accuracy(predictions, batch.labels)\n",
        "            acc = categorical_accuracy(predictions, batch.labels)   \n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHbnFldcMK3m"
      },
      "source": [
        "Let's Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGjPdtxXg_R5",
        "outputId": "46108947-57ab-4adc-a8d0-9b70e3d05894"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.546 | Train Acc: 32.11%\n",
            "\t Val. Loss: 1.575 |  Val. Acc: 28.27% \n",
            "\n",
            "\tTrain Loss: 1.492 | Train Acc: 39.02%\n",
            "\t Val. Loss: 1.546 |  Val. Acc: 33.29% \n",
            "\n",
            "\tTrain Loss: 1.444 | Train Acc: 44.58%\n",
            "\t Val. Loss: 1.562 |  Val. Acc: 31.75% \n",
            "\n",
            "\tTrain Loss: 1.408 | Train Acc: 48.54%\n",
            "\t Val. Loss: 1.548 |  Val. Acc: 33.57% \n",
            "\n",
            "\tTrain Loss: 1.373 | Train Acc: 52.23%\n",
            "\t Val. Loss: 1.566 |  Val. Acc: 32.97% \n",
            "\n",
            "\tTrain Loss: 1.341 | Train Acc: 55.32%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.97% \n",
            "\n",
            "\tTrain Loss: 1.315 | Train Acc: 58.20%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 34.18% \n",
            "\n",
            "\tTrain Loss: 1.291 | Train Acc: 60.67%\n",
            "\t Val. Loss: 1.558 |  Val. Acc: 34.00% \n",
            "\n",
            "\tTrain Loss: 1.275 | Train Acc: 62.22%\n",
            "\t Val. Loss: 1.555 |  Val. Acc: 33.58% \n",
            "\n",
            "\tTrain Loss: 1.257 | Train Acc: 64.31%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 34.97% \n",
            "\n",
            "\tTrain Loss: 1.238 | Train Acc: 66.12%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 35.48% \n",
            "\n",
            "\tTrain Loss: 1.223 | Train Acc: 67.72%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 35.06% \n",
            "\n",
            "\tTrain Loss: 1.213 | Train Acc: 68.71%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 34.71% \n",
            "\n",
            "\tTrain Loss: 1.200 | Train Acc: 69.98%\n",
            "\t Val. Loss: 1.549 |  Val. Acc: 34.28% \n",
            "\n",
            "\tTrain Loss: 1.191 | Train Acc: 70.89%\n",
            "\t Val. Loss: 1.541 |  Val. Acc: 35.55% \n",
            "\n",
            "\tTrain Loss: 1.180 | Train Acc: 72.07%\n",
            "\t Val. Loss: 1.520 |  Val. Acc: 38.33% \n",
            "\n",
            "\tTrain Loss: 1.170 | Train Acc: 73.18%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 35.40% \n",
            "\n",
            "\tTrain Loss: 1.162 | Train Acc: 74.00%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.35% \n",
            "\n",
            "\tTrain Loss: 1.156 | Train Acc: 74.56%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.85% \n",
            "\n",
            "\tTrain Loss: 1.148 | Train Acc: 75.40%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 37.21% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTGr3h6WhCht",
        "outputId": "fad69abe-0b27-4dde-e81b-0b87bd3fc31e"
      },
      "source": [
        "model.load_state_dict(torch.load('saved_weights.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.533 | Test Acc: 35.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiNwU_nZhGET"
      },
      "source": [
        "path = './sentiment_analysis_saved_weights_1.pt'"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "9Nu9eC6ThZsk",
        "outputId": "93e673e0-de77-4882-9aab-136c5365648a"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path = './sentiment_analysis_saved_weights_1.pt'\n",
        "# path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    # labels: very negative, negative, neutral, positive, very positive\n",
        "    categories = {1: \"very negative\", 2:\"negative\", 3:\"neutral\", 4:'positive', 5: 'very positive'}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-7bde54b38824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./sentiment_analysis_saved_weights_1.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# path='./saved_weights.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sentiment_analysis_saved_weights_1.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sSuBOm8hcHD"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDx5bJI0hgxr"
      },
      "source": [
        "Random Insertion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnUtjnHAheeo"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MMqPlFChow-"
      },
      "source": [
        "Random Deletion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm36o0TMhq5t"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bVClNxUhtio"
      },
      "source": [
        "Random Swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGpiGevAh2iB"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdAoCbEKh5cA"
      },
      "source": [
        "Back Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFXINSn_h7Ew"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        "import googletrans.Translator\n",
        "\n",
        "translator = Translator()\n",
        "sentence = ['The dog slept on the rug']\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "t_text = [t.text for t in translations]\n",
        "print(t_text)\n",
        "\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "print(en_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}